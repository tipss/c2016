
===============
Hypervisor is a software. Once installed, this software help  create and maintain virtual machines that will share physical resource,
Not needed to create containers.(Containers share common Kernel API, and run in User space as an isolated process

VM's need full copy of OS, and each VM has to have one OS, and on a single Baremetal server with Hypervisor installed, VM's OS can be mix and match.
Hypervisor types:
Type 1 : Installed on bare metal as OS (e.g ESXI vSphere, Citrix XEN, KVM , Miscrosoft Hyper-V)
         Generally in data centers use this.
Type 2 : Install on top of Host OS as softwares
       > it can be linux or windows host.
         e.g vmware fusion, Oracle virtual box,  paralles
>-
virtualized networking , how VMs or containers communicate each other ?.
Your physical NIC needs be mapped or shared across multiple VMs to make VM's talk to out side world.

How to acheive that ?.
Typically , on Hypervisor, we create Open vSwitch. In this vSwitch we map veth from
VM into a physical NIC with vlan(running a bridge). Thus a single physical NIC can be used by multiple
VM's with vlan as seperator, and without VM's knowing that what is the other VM using this physical NIC.

How VM's to VM communicate if subnets are in different networks?.
Veths if they are in different network, create vRouter, adn add veths into that vRouter to route
these networks.

Virtulization Use Cases.
Server virtualization:
e.g Migrate single purpose hosts(e.g dhcp server, dns server etc etc, db server) to VM.
    Then host VM via Hypervisor on a single baremetal server.


VDI: Virtual Desktop Infra:
How user client machine on a VM.(think clients, because they have bare minimal OS infra), no info
is stored on the thin-clients. Secure,

==========================================================

APS-T-NDP: App Presen, Session, Transport Network Data Link Phy
A HTTP, Radius
P
S  DNS, LDAP
T  TCP, UDP
N  ICMP, RSVP, IPv4,IPv6, PIM, VRRP
D: ARP, L2 protosl
P

When to use threads, vs when to use process?.

http://www.cafeaulait.org/course/week11/02.html
    Both threads and processes are methods of parallelizing an application. 
However, processes are independent execution units that contain their own state information, 
use their own address spaces, and only interact with each other via interprocess communication mechanisms 
(generally managed by the operating system). Applications are typically divided into processes 
during the design phase, and a master process explicitly spawns sub-processes when it makes sense 
to logically separate significant application functionality. Processes, in other words, are an architectural construct.

    By contrast, a thread is a coding construct that doesn't affect the architecture of an application. 
A single process might contains multiple threads; all threads within a process share the same state 
and same memory space, and can communicate with each other directly, because they share the same variables.

    Threads typically are spawned for a short-term benefit that is usually visualized as a serial task, 
but which doesn't have to be performed in a linear manner (such as performing a complex mathematical 
computation using parallelism, or initializing a large matrix), and then are absorbed when no longer 
required. The scope of a thread is within a specific code module—which is why we can bolt-on 
threading without affecting the broader application.



Signal Handler?, Why we need them, and when we need them.

/* Life cycle of Server socket : socket(), bind(), listen(), while(select(fd_list)) accept() read() and/or send() .. listen()) */
/* Life cycle of Client socket : socket(), bind(), connect(), write() close(), repeat if needed*/

What is a Linux Socket?.
Socket in linux is an end point  for communication, 
sock_id= socket(domain, type,protocol)
e.g domain=(AF_NETLINK, PF_ROUTE, PF_UNIX/PF_LOCAL etc)
e.g type SOCK_STREAM: reliable connection based, e.g TCP
         SOCK_DGRAM: connectionless, unreliable messages UDP.
         SOCK_RAW: 
What is a Linux bind(sockfd, sockaddr, len) mean?
When socket is created using 'socket()', it exists in a name space,
but has no address assigned to it. bind() assigns the address specified by addr
to the socket to the sockfd. its also called assigning a name to a socket.
After socket is bound, a buffer size is set.

What is listen() ?. when to use.
After socket is bound, we have to listen() on that socket via 
blocking(poll) calls, then accepts(mostly done in server)

When there is a an incoming connection, we can accept() peers
connection via that socket.
 listen() marks the socket referred to by sockfd as a passive socket,
that is as a socket that will be used to accept incoming connection request using accept()

When to use read() ?.
After accept() by a passive socket we start read() on that
socket. When read is done, or nothing to read, we wait in 'accept()
while loop.


Clients act as producer, 
Server act as consumer, both running in different processes.

Server socket sequence is like below:
socket(),
bind(),
listen(),<----make it a passive , so ready to accept connection
while(1) {  <-------select() is used here to wait on multiple client connection on different sockets
  accept()
  read()
}
close()<-----End of server

Client socket sequence is like below:
socket()
bind()
connect() to the PORT?addres used in server.
write();


These are both POSIX functions, and their standard definitions are:

sched_yield() "The sched_yield() function shall force the running thread to relinquish the processor until it again becomes the head of its thread list." This function has a standard C++ analog, std::this_thread::yield().
sleep() "The sleep() function shall cause the calling thread to be suspended from execution until either the number of realtime seconds specified by the argument seconds has elapsed or a signal is delivered to the calling thread"

If you call sleep(0), the implementation is allowed to do exactly nothing and return immediately. Linux implementation of sleep(0) actually does exactly that, nothing. The code literally says


What is the difference between sleep() vs pthread_yield() ?
sleep: makes the calling process sleep for the duration given, no other thread is scheduled
pthread_yield: calling thread politely give a way for other waiting threads to
run if there are waiting. This is done by relinquishing use of procesor,
and moving itself to run-queue for its turn to come.

What does 'select() do' ?.
https://www.lowtek.com/sockets/select.html
at your server cannot act like there's only one client, like with a fork()'ing solution. For example, with a fork()'ing solution, after the server fork()s, the child process works with the client as if there was only one client in the universe -- the child does not have to worry about new incoming connections or the existence of other sockets. With select(), the programming isn't as transparent.

Okay, so how do you use select()?

select() works by blocking until something happens on a file descriptor (aka a socket). What's 'something'? Data coming in or being able to write to a file descriptor -- you tell select() what you want to be woken up by. How do you tell it? You fill up a fd_set structure with some macros.

Most select()-based servers look pretty much the same:

    * Fill up a fd_set structure with the file descriptors you want to know when data comes in on.
        * Fill up a fd_set structure with the file descriptors you want to know when you can write on.
	    * Call select() and block until something happens.
	        * Once select() returns, check to see if any of your file descriptors was the reason you woke up. If so, 'service' that file descriptor in whatever particular way your server needs to (i.e. read in a request for a Web page).
		    * Repeat this process forever. ~



Why to use 'select() before read(on socket)?.

Kubernetics:
Container orchestration tools are : kubernetics, docker swarm, mesos

What are Dockers?(also called Docker Container Virtulization).

Solves this problem for everone, by making an image of an entire application,
with all its dependencies and ship it to your required target environment / server.
So in short, if the app worked in your local system, it should work anywhere
in the world(because you are shipping the entire thing).

The above problem can be solved using VM. But Dockers are very light weight, size, time etc.

=============================
Linux Namespace and CGroups

Its a isolated networking environments running on a single physical host or VM.
It has its own interface, routing table and forwarding table, 
process id lists, network devices, file system, user lists etc
Process can be dedicated to one network namespace.
Used in Linux Containers, OpenStack, Mininet, Docker etc.

Root Namespace: Its the default network Namespace, all other namespaces are launched from this.
E.g add new namespace:
ip netns add red,
ip netns add green,
ip netns,
ip netns
ls /var/run/netns/
======================================================
Cgroups:(Linux kernel constuct)
It meters resources.

By default on a Linux system,
1. all processes are children of the INIT process.
   Which means all processes are part of a single tree structure.
   Now in Cgroup method, different process groups can exist on a single system.
   So instead of a single process tree of default linux method,
   cgroup method can have different trees of process structure
   (with different parents, and childs will inherit stuff from their parents),
   all isolated from each other.
   Now you might have got an idea of how cgroups and namespace are
   leveraged by container based virtualization. 

VRF-lite:(New thing as opposed to usng namespace based VRF's in FRR)
https://www.kernel.org/doc/Documentation/networking/vrf.txt

Virtual Routing and Forwarding (VRF)
====================================
e.g : Created inside container.
ip link add <vrf-name> type vrf table <table-no>
ip link set swp1 master vrf-red

Simmilar to 'bridge' we create vrf in linux.
Bind socket to l3 domain.
VRF is L3 concept(route lookups, storing)
Kernel should support this feature.
netdevice : allows use of netfllter, traffic control (tc) rules.
tcpdump etc

The VRF device combined with ip rules provides the ability to create virtual
routing and forwarding domains (aka VRFs, VRF-lite to be specific) in the
Linux network stack. One use case is the multi-tenancy problem where each
tenant has their own unique routing tables and in the very least need
different default gateways.

Processes can be "VRF aware" by binding a socket to the VRF device. Packets
through the socket then use the routing table associated with the VRF
device. An important feature of the VRF device implementation is that it
impacts only Layer 3 and above so L2 tools (e.g., LLDP) are not affected
(ie., they do not need to be run in each VRF). The design also allows
the use of higher priority ip rules (Policy Based Routing, PBR) to take
precedence over the VRF device rules directing specific traffic as desired.

In addition, VRF devices allow VRFs to be nested within namespaces. For
example network namespaces provide separation of network interfaces at the
device layer, VLANs on the interfaces within a namespace provide L2 separation
and then VRF devices provide L3 separation.

What is a Swap?.
Swap is a filesystem type – a raw disk partition is formatted as swap upon
system installation. It's treated as second-level RAM by the OS.
When the OS runs out of RAM, it uses swap. As a rough heuristic,
system administrators sometimes configure the size of the swap partition to be twice that of available RAM

What is Virtual Memory(Swap)?
Simply put, virtual memory is a combination of RAM and disk space that running processes can use.
Swap space is the portion of virtual memory that is on the hard disk, used when RAM is full.

With the use of Virtual Memory, each process is isolated to its own memory space thus isolating each process .

What is valgrind?
Valgrind uses a technology called dynamic binary instrumentation (DBI) to instrument code.
Usage : valgrind df
Generates a report of dynamic memory allocation used by program 'df'.

What's a core dump anyway?
A core dump is a snapshot of certain dynamic regions (segments) of the process at the time it crashed (technically,
it's a snapshot of minimally the data and stack segments).
The core dump can be analyzed postmortem using debuggers such as GDB

As for why 32bit CPU is limited to 4gb virtual memory ?.
By definition, a 32-bit processor uses 32 bits to refer to the location of each byte of memory. 
2^32 = 4.2 billion, which means a memory address that's 32 bits long can only refer to 
4.2 billion unique locations (i.e. 4 GB).

Very good read here for windows OS :
https://support.microsoft.com/en-us/kb/294418
==============================================================

What is a shared memory ?.
One way to communicate between two process is by using shared memory.
One process writes, other process reads.

Also shared memory is used during restarts, as its non-volatile(exist after process dies).



You cannot share data structures containing pointers. This is because each process has its own virtual address space, and a pointer from one process doesn't have any meaning in another process.

If you still want to share linked data structures (e.g trees, lists) you must define
a serialization protocol, usually instead of pointers use indexes relative to the
start of the shared memory area.


How to create and use shared memory?.
Important functions:
ftok   get a numeric key for a file
shmemget use key associated with a file name, and returns a shared memory block.
shmat: attach to the block reteruned above,
      map the block id into a pointer , so it can be used.
shmdt: detach

What is mmap?.

=============================
VM Migration:
============================
Virtulization(Hypervisor based)
Its a way to run an OS(Guest) on top of another OS(Host).
Mostly acheived using 'Hypervisor software'.
Job of Hypervisor is to 'emulate' underlying physical hardware and show it to Guest-OS.

Hosted Virtualization: Hypervisor is installed as software app in Host-OS(which is 
running on Bare metal).e.g VirtualBox, VMware Workstation Microsofts Virtual PC
Performance of these are not very good, due to the fact that there are 
1. multiple Memory and CPU managers running for each 
VM(1 is added from HOST, 1 is added from Hypervisor, 1 is added from Guest OS). 
2. Device Drivers are not part of this Hypervisor.

Baremetal Virtulization: Hypervisor runs directly on Baremetal like(OS).
Device Drivers are part of this software. There will be only one CPU and Memory manager for the 
entire physical hardware. so performance will be at its best.

Virtualization(Container Based):
Container space is part of the linux Kernel feature cgroups and namespaces. 
So each container share the same kernel.
So much lightweight than Hypervisor based virtulization, but isolation is limited?.
But there can be many containers hosted per OS. VM's have limits.
As the container is sharing the kernel with the base system, you can see the processes
that are running inside the container from the base system.
However when you are inside the container, you will only be able to see its own processes. 
===========================
GDB: Tool used to debug a running or core of a program.

S/W breakpoints:
Running program will be stopped at this address location 
when PC(Program Counter) hits this address. GDB will change instruction around this 
area to generate 'trap', or 'exception' or 'illegal inst',so
when PC runs over this, GDB will receive this signal and GDB hands over the control to USER who
is debugging. 
It also restores the BREAK POINT, if you remove the breakpoint.
Program area must be 'writable' for GDB to work, so it will not work on ROM areas.

H/W Breakpoints:
Designed within the Chipset to store Break-Point address in special register.
These registers are monitored by the chip for running PC . if they hit, a control is given
back to user of GDB.

Watchpoints:
Special kind of breakpoints, triggered when data is accessed, rather than some instructions
are executed.
When to use watchpoints?,
When some data changes its value, without your knowledge, its better to set watchpoint to find
who is actually changing the data.

In case of debugger: parent process is gdb, child process is the debuggee.
Parent process have additional powers over child process, one of the power is to run 'ptrace' on them.
ptrace allows a parent process to access low-level info about child process.

ptrace is part of linux kernel.



Multithreading Terms
There are many terms used when writing multithreaded applications. I'll try to describe a few of there here.

Deadlock — A state where two or more threads each hold a lock that the others need to finish. 
For example, if one thread has locked mutex A and needs to lock mutex B to finish, 
while another thread is holding mutex B and is waiting for mutex A to be released, 
they are in a state of deadlock. 
The threads are stuck, and cannot finish. 

t1 mutexA wait for mutexB
t2 mutesB wait for mutexA

How to avoid deadlock ?.
1. One way to avoid deadlock is to acquire necessary mutexes in the same order (always get mutex A then B). 
2. Another is to see if a mutex is available via pthread_mutex_trylock, 
   and release any held locks if one isn't available.

Race Condition — A program that depends on threads working in a certain sequence to complete normally. 
                 Race Conditions happen when mutexes are used improperly, or not at all.

Thread-Safe — A library that is designed to be used in multithreaded applications is said to be thread-safe. 
If a library is not thread-safe, then one and only one thread should make calls to that library's functions.

Multithreaded applications often require synchronization objects. 
These objects are used to protect memory(or critical region) from being modified by multiple 
threads at the same time, which might make the data incorrect.
e.g : Mutex, semaphore
The first, and simplest, is an object called a mutex. A mutex is like a lock. A thread can lock it, and then 
any subsequent attempt to lock it, by the same thread or any other, will cause the attempting thread to block 
until the mutex is unlocked. 

These are very handy for keeping data structures correct from all the threads' 
points of view.
For example, imagine a very large linked list. 
If one thread deletes a node at the same time that another thread is trying to walk the list, 
it is possible for the walking thread to fall off the list, so to speak, 
if the node is deleted or changed.
Using a mutex to "lock" the list by both threads  protects  from happening.

Mutex =  Mutual Exclusion.
One thread holds this lock(winner thread) excluding other threads(loosers).
Why we need mutex?.
Linux has critical sections which are shared writable address space.
without the use of mutex, multiple threads can write to the same critical section at the same time,
which is not ok.

In Java, Mutex-like behaviour is accomplished using the synchronized keyword.

Technically speaking, only the thread that locks a mutex can unlock it, but sometimes operating 
systems will allow any thread to unlock it. Doing this is, of course, a Bad Idea. 
If you need this kind of functionality, read on about the semaphore in the next paragraph.

Similar to the mutex is the semaphore. A semaphore is like a mutex that counts instead of locks. 
If it reaches zero, the next attempt to access the semaphore will block until someone 
else increases it. This is useful for resource management when there is more than one resource, 
or if two separate threads are using the same resource in coordination. 
Common terminology for using semaphores is "uping" and "downing", where upping increases the count 
and downing decreases and blocks on zero. Java provides a Class called Semaphore which does 
the same thing, but uses acquire() and release() methods instead of uping and downing.

Unlike mutexes, semaphores are designed to allow multiple threads to up and down them all at once. 
If you create a semaphore with a count of 1, it will act just like a mutex, 
with the ability to allow other threads to unlock it.

Mutex always has owner(for e.g thread). So no other owner can operate on them.
This helps issues arising out of priority inversion etc.
Ch15.
What kernel version we support in nvOSd?.
4.18 linux kernel,
What ubuntu version we support
16.04 xilix, latest is 18.04 bionic(which we do not support)

CPU we use in h/w samples e.g ?
Broadwell: Intel Xeon® CPU    D-1557 @ 1.50GHz (12C)  NRU 03
Intel(R) Atom(TM) CPU  C2558  @ 2.40GHz  Phoenix,
MFR(assembly) : Dell, Accton, Celestica,   mSATA SSD, RAM 8g to 16G
Switch chipset: Alta(Freedom series),  Trident2, Tomahawk, Trident2+
Form factor 2U most of them

reader-writer mutex lock?.
pthread_wrlock_t,
A thread request for read-lock, which is always granted.
A thred  request for write, at this time normal lock contention etc can happen, as it involves usual checks.
recursive mutex?.
Yes its possible to take mutex lock recursively, if mutex is created with attribute set PTHREAD_MUTEX_RECURSIVE,
while creating mutex.

Mutexes can be applied only to threads in a single process and do not work between processes as do semaphores. 
semaphores are primarily used for process syncronization.
mutex are for thread syncronication.

What is self deadlock?.
A thread taking same lock agin while holding already lock for it.
Its not allowed(recursive) in linux by default unless a variable is set
to allow such locks. But an attempt by code creates deadlock.

How to commmunicate between two 'process' in linux in a syncronouse way ?.

Why we need condition variable ?.
If a thread want wait until certain work is done, it can wait on a condition variable.
The other thread can touch this condition variable and signal the waiting thread to continue to work.
Without the use of condition-varibale, both threads access same resource leading to
race condition,

Basically a thread can be put in a wait queue, by making it wait on a condition variable.
It gets scehuled only after that condition variable is ready to be used.

/usr/local/
One way is to use pthread condition variable.

pthread_cond_t c = PTHREAD_CONDITION_INITIALIZER;
pthread_mutex  m = PTHREAD_MUTEX_INITIALIZER();
int done = 0;

void *child (void *){
       pthread_mutex_lock(&m);
       done = 1;
       pthread_cond_signal(&c);
       pthread_mutex_unlock(&m);
}
void *parent () {
       pthread_mutex_lock(&m);
      if (done != 1) {
	  pthread_cond_wait(&c, m)<-----releases lock as-soon as it starts wait, so other threads can work on 'done'.
	  			        when the wait ends due to receive of signal, it will continue to hold the mutex.
	                                This will help to make this an atomic operation before wait.
       }
       pthread_mutex_unlock(&m);
       return NULL;

}

int main (int argc, char *argv[]) {
pthread_t p;
	  pthread_create(&p, NULL, child, NULL);
	  parent();
		
}


The pthread_cond_wait(&cond_var, &mutex)  function can be used to block on a condition variable. 
They are called with mutex locked by the calling thread or undefined behaviour will result.
These functions "atomically" release mutex and cause the calling thread to block on the 
condition variable cond; atomically here means "atomically with respect to access by 
another thread to the mutex and then the condition variable". That is, if another thread is 
able to acquire the mutex after the about-to-block thread has released it, 
then a subsequent call to pthread_cond_signal() or pthread_cond_broadcast() 
in that thread behaves as if it were issued after the about-to-block thread has blocked.

Upon successful return, the mutex has been locked and is owned by the calling thread.

Typical Implementation of IPC library in a Router running on Linux with restartable 
process for each protocol: Each process can implement an IPC library,which when 
initialized will creates a thread and waits on select. When socket finds a FD-SET due to 
'ipcWrite' by remote end, it will do ipcReceive() on that socket, then goes back to loop.

The select waits on a port, which is advertised by ipcRegister(END_POINT) by this IPC library.
ipcRegisters registers this END_POINT with Process Monitor, whic is a process keeps track of all end points by name.

IPC thread also sends priodic hellos to PM, otherwise PM will kill this process.

END_POINT is typically made up of : IP-Address+Ip_port+index+instance
Where in, index represents thread ID, Ip_port identifies process in the domain.

AF_IPC type of sockets are used to interconnect between two process within same card.
UDP sockets are typically used to communicate between standby Cards.


Also, of a process wants to create one more ENDPOINT so it can establish another port, it should do this
using another thread. Register this endpoint with another name using ipcRegister, 
This helps keep everything seperated and clean for multi-thread applications.

for every ipcSend, a copy is sent to kernel space.
ipcSend is ACK based, and ACK's are kept track in IPC library, which watch for 32 unacked buffers.
If no-ack , it will re-transmit, so a re-transmit queue needs be maintained.


Simple pthread example:
#include <pthread.h>
void *myturn(void *) {
      while(1) {
      	       sleep(2);
               printf("%s\n",__FUNC__);
      }
}

void *yourturn(void *) {
      while(1) {
      	       sleep(2);
               printf("%s\n",__FUNC__);
      }
}
int main(int argc, char *argv[]) {
pthread_t t;
	  pthread_create(&t, NULL, &myturn, NULL);
	  yourturn();
}



What is a network name space vs VRF ?.
Has logical representation of network, has its own
routing table, iptable etc.

How to look in network namespace,

>ip netns exec <blue-ns> ip a
>ip netns exec <blue-ns> ping xxx
>ip netns exec <blue-ns> iptable -L



Floatig IP in openstack?.
Are createad when a new instanace(vm) is added in a network,

How to share Host OS vs Container OS files?
e.g container name p1

Well, let’s say we want to have our host’s /var/cache/lxc
shared with “p1”, we can edit /var/lib/lxc/p1/fstab and append:

/var/cache/lxc var/cache/lxc none bind,create=dir


Crash-only software refers to computer programs that handle failures by simply restarting, without attempting any sophisticated recovery.[1] Correctly written components of crash-only software can microreboot to a known-good state without the help of a user. Since failure-handling and normal startup use the same methods, this can increase the chance that bugs in failure-handling code will be noticed, except when there are leftover artifacts, such as data corruption from a severe failure, that don't occur during normal startup.

Crash-only software also has benefits for end-users. All too often, applications do not save their data and settings while running, only at the end of their use. For example, word processors usually save settings when they are closed. A crash-only application is designed to save all changed user settings soon after they are changed, so that the persistent state matches that of the running machine. No matter how an application terminates (be it a clean close or the sudden failure of a laptop battery), the state will persist.


NETLINK:
IPC between userspace process and kernel
Primarily used to configure linux kernel, like routes, interfaces via an IPC from user-space
Replacement for ioctl(oldstyle).
Its an IPC interface.
famalies: rtnetlink : used for interface and routes. bridge-fdb
          genetlink : generic extensible netlink family.
	  nfnetlink : kernel netfilter system
rtnetlink is route-netlink(NETLINK_ROUTE) used for to configure network interfaces(links), addresses, routes etc
message_type: RTM_NET, _GET, _SET, _DEL
How user space use this socket ?.
netlink_socket =socket(AF_NETLINK, sock_type, netlink_family);
Create socket(), bind(), sendmsg() recvmsg().
Header : Fixed Part, Lenght Type Value(LTV) array .

User can register at user-space for async link notification(routesnoop)


vmstat:  Report virtual memory statistics, paging, block IO, traps , discks and cpu activity
ltrace : library calltrace
stract: system calls trace
pidstat: Process stats
lsof: List Open Files,    : Files may be regular file, directory, block special file, stream, network file.
mpstat: better than vmstat
iostat: Report CPU stats and i/o stats for devices and partition
ip -s address show
ip neighbor
ip tcp_metrics
ip netlink
ip route

ss : Dump socket statistics
sar: system activiti report
